# Capitolo 2.

## Lavorare con i dataset

Un'astrazione importante in PyTorch è costituita dal Dataset.
La classe che incapsula un nostro dataset deve:
* estendere la classe torch.utils.data.Dataset
* implementare i due metodi: `__len__` e `__get_item__` (due underscore)

Nel Notebook [prepare_dataset1](./prepare_dataset1.ipynb) mostro come possiamo incapsulare in un Dataset il caricamento di un file csv ed anche la logica di trasformazione dati, in modo da avere i dati pronti per essere passati in input ad una rete neurale FC implementata in PyTorch.

Il concetto è del tutto generico e vale anche nel caso di immagini, audio, etc: ogni volta che dovremo lavorare con un nostro dataset dovremo scrivere una tale classe implementando i due metodi:
* `__len__` e `__get_item__`

## Items come torch.tensor

PyTorch consente di lavorare e mescolare vettori Numpy e tensori (torch).

Ma, si devono avere presenti alcune fondamentali differenze:
1. I tensori PyTorch possono essere utilizzati anche sulle **GPU** e le operazioni sono eseguite sulle GPU (basta spostare i tensori sulla GPU)
2. Numpy non può lavorare su GPU
3. Se il codice utilizza Numpy vi possono essere difficoltà nel momento in cui si vuole utilizzare ONNX per la distribuzione (deploy)

Per questi motivi, io penso che, ad esempio se si scrive una classe Dataset custom, si deve fare attenzione a produrre in uscita tensori e non liste o vettori numpy.
Ciò, per quanto concerne l'addestramento del modello, dove vogliamo poter usare le GPU.

Per quanto riguarda l'inferenza, dopo aver prodotto le previsioni (mediante applicazione del modello) possiamo trasformare i risultati in vettori Numpy, se utile.

## DataLoader

Se si vuole scrivere a mano (quindi non usare Lightning) l'intero codice per il **training loop**, si devono gestire aspetti quali lo shuffling ed il batching dei record.

Lo **shuffle** è un aspetto importante, che non va dimenticato: nelle varie epoch è fondamentale che i vari sample siano presentati, alla rete da addestrare, in un ordine casuale.

Inoltre, sopratutto se si utilizzano le GPU, è molto utile passare in input alla rete i dati in **mini-batch**, la cui dimensione deve essere scelta con accuratezza.

I **DataLoaders** di PyTorch si prendono cura di questi aspetti. Quindi, non dobbiamo fare altro che usare la classe **DataLoader** di **torch.utils.data**, passando gli opportuni parametri (shuffle, batch_size).

Il [notebook](./data_loaders.ipynb) contiene un esempio che mostra come costruire il DataLoader e come iterare sui vari batch.

## Esempi

Nel Notebook [prepare_dataset1](./prepare_dataset1.ipynb) mostriamo come creare un Dataset per dati tabellari.

Nel Notebook [image_folder_dataset](./image_folder_dataset.ipynb) mostriamo come creare un Dataset per un task di classificazione immagini, a partire da immagini **suddivise in folders** (uno per classe)

Nel Notebook [multi_label_hbku201_prepare_dataset](./multi_label_hbku2019_prepare_dataset.ipynb) mostriamo come creare un Dataset per un task di classificazione **multi-label**. Le label sono fornite in un file CSV.


