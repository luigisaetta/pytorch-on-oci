# Capitolo 1

## Introduzione.
* TODO

## Creazione di un ambiente PyTorch

Il primo passo da compiere per utilizzare PyTorch in OCI Data Science è di creare una Notebook Session.

La **Notebook Session** la possiamo pensare come una macchina virtuale (VM), con il numero di core, la memoria e 
lo spazio di memorizzazione specificati all'atto della creazione (la sua "shape").

In realtà, si tratta di un'infrastruttura più complessa, che fornisce una serie di funzionalità aggiuntive e di benefici, rispetto ad 
una VM custom.
La VM tipicamente (è una best practice) è collocata su una subnet privata. L'accesso tramite questa subnet è utilizzato per il traffico "outbound", ovvero 
per le richieste in uscita dalla VM. Esempio: l'accesso agli altri servizi Cloud (come l'Object Storage) oppure le fonti di dati (un Autonomous DWH).
L'accesso in ingresso, per utilizzare tramite Browser l'interfaccia tipica di **Jupyter**, avviene attraverso una serie di layer, gestiti da Oracle, che garantiscono la sicurezza (possiamo immaginare LB, proxy, ..).
Inoltre, una Notebook Session è un servizio **Oracle Managed**. Non dobbiamo preoccuparci di monitorarlo, E' compito delle Cloud Operations Oracle di garantire che sia up&running, nel rispetto dei Livelli di Servizio (SLA) contrattuali.

Nel momento in cui creiamo la Notebook Session abbiamo una scelta abbastanza ampia. Possiamo crearla in prima istanza con pochi core e poi, semplicemente disattivando e riattivando con un'altra shape, aumentare la capacità di calcolo a disposizione, quando necessario.

Negli esempi che mostro, spesso utilizzerò una **GPU** per velocizzare le elaborazioni. La shape che quindi ho scelto è una **VMGPU2.1**, equipaggiata con una GPU economica (P100) ma sufficiente per i casi che illustrerò.

Dopo aver creato la Notebook Session il passo successivo da compiere è di creare l'ambiente software.
Il package manager utilizzato è Anaconda (conda).
Creare un ambiente integrato, con centinaia di librerie a disposizione, di versioni compatibili, non è per nulla un'operazione semplice.
Ma, a ciò ha pensato il team di Oracle Data Science, che mette a disposizione, pronti da installare, più di **45 ambienti conda** (ed il numero cresce mese per mese).

L'ambiente che ho selezionato per i miei primi esempi è l'ambiente: **PyTorch 1.10 for GPU on Python 3.7**.
Preciso che tale ambiente, pur mettendo a disposizione le librerie dell'ecosistema PyTorch integrate con le CUDA di NVIDIA, può essere utilizzato anche all'interno di una
NB session senza GPU.

Dall'Environment Explorer posso copiare il comando che consente di installare tale ambiente:

```
odsc conda install -s pytorch110_p37_gpu_v1 
```

e poi possiamo eseguire tale comando in una finestra di terminale (io preferisco per tali operazioni la command line).

L'installazione dura poco, circa tre minuti. Dopo i quali possiamo attivare nella finestra di terminale l'ambiente con il comando (che è visualizzato al
termine dell'installazione):

```
conda activate /home/datascience/conda/pytorch110_p37_gpu_v1
```

## Un primo test dell'ambiente PyTorch

A questo punto siamo pronti per un primo test dell'ambiente. Io suggerisco sempre, prima di lanciarsi nello sviluppo di un modello complesso di rete neurale, di effettuare un semplice check-up dell'ambiente.

Ho preparato un Notebook che può aiutare nello scopo. Nell'ordine:
* visualizza la versione di PyTorch installata (allo stato attuale, la vers. 1.10)
* controlla che l'integrazione con le CUDA sia funzionante
* controlla il numero di GPU (nel caso della VMGPU2.1 1 GPU)
* visualizza altre utili informazioni, tramite il comando nvidia-smi

Ecco il [link](./check_pytorch_and_gpu.ipynb) al Notebook.

Un punto di attenzione: prima di eseguire il Notebook dobbiamo assicuraci che il kernel utilizzato sia quello che utilizza il nostro ambiente sw per PyTorch.
Dopo aver aperto il Notebook nel broser, il kernel è visualizzato nell'angolo in alto a destra.

## Le librerie installate.

L'ambiente conda per PyTorch contiene moltissime librerie: ne ho contate **276**, con il comando:
```
conda list |wc -l
```
Tra le più importanti e note io elencherei:
* bokeh
* cx-oracle
* pandas
* torch (ovviamente)
* matplotlib
* seaborn
* optuna

Ovviamente, non tutte le librerie possono essere presenti o possono essere aggiornate alle ultime versioni disponibili.

Ma **possiamo aggiornare l'ambiente**, utilizzando **conda** (da preferire) o **pip**.
L'unico requisito importante è che la NB session sia collegata, tramite [NAT Gateway](https://docs.oracle.com/en-us/iaas/Content/Network/Tasks/NATgateway.htm), ad Internet in modo da poter "uscire" ed accedere ai repository pubblici di librerie Python (come i canali conda o PyPi)

Ad esempio, con il comando:
```
pip install pytorch-lightning
```
io vado ad installare il pacchetto [PyTorch Lightning](https://www.pytorchlightning.ai/), che esploreremo nelle sezioni più avanzate. Vi anticipo che esso permette di semplificare notevolmente, e di standardizzare, la scrittura di codice basato su PyTorch.

Un altro pacchetto che in genere installo è **black**, che utilizzo per formattare meglio il codice Python:
```
pip install black
pip install black[jupyter]
```

Ultimi due pacchetti: 
* **PyTorch TabNet**, il cui impiego sarà descritto, con ampi dettagli, nel capitolo dedicato
* **imbalanced-learn**.

Per TabNet, è importante effettuare l'installazione utilizzando conda. La versione utilizzata è la 3.1.1.
```
conda install -c conda-forge pytorch-tabnet
pip install imbalanced-learn
```

Conviene, per finire, effettuare l'upgrade della libreria **Oracle ADS**:
```
pip install oracle-ads -U
```

la versione che utilizziamo, dopo l'upgrade, è la 2.6.6

## Dataset di esempio.

Per sviluppare gli esempi, abbiamo bisogno di dataset che ci consentano di evidenziare le funzionalità interessanti, in generale di PyTorch e di OCI Data Science.
Per gli esempi su **dati strutturati, tabellari**, utilizzerò alcuni dataset che sono resi disponibili, all'interno della Notebook Session, nella directory:

```
/opt/notebooks/ads-examples/oracle_data
```

In particolare, per i primi esempi relativi allo sviluppo di un modello di classificazione (binaria) utilizzerò il dataset contenuto nel file
```
orcl_attrition.csv
```


