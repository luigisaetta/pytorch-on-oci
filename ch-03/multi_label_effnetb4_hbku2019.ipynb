{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4780c49b",
   "metadata": {},
   "source": [
    "### Example of Multi-label classification with PyTorch\n",
    "\n",
    "Multi-label means that the same image can have associated several labels.\n",
    "\n",
    "In other words: class are not exclusive\n",
    "\n",
    "we will be using the **HBKU2019 dataset (from Kaggle)**\n",
    "\n",
    "In this NB we're using **EffNet B4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a274d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92414bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# globals\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "BASE_DIR = \"/home/datascience/hbku2019-dataset/\"\n",
    "# file with labels\n",
    "DATA_CSV = BASE_DIR + 'labels/labels_train.csv'\n",
    "# dir with images\n",
    "IMAGE_DIR = BASE_DIR + \"imgs/train/\"\n",
    "\n",
    "# Img model input size\n",
    "IM_SIDE = 260\n",
    "\n",
    "# reading the categories\n",
    "cats = pd.read_csv(BASE_DIR + 'labels/categories.csv', header=None)\n",
    "cats = list(cats[0])\n",
    "\n",
    "# Number of classes\n",
    "N_CLASSES = len(cats)\n",
    "\n",
    "print(cats)\n",
    "print(len(cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbdae30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetFromCSV(Dataset):\n",
    "    def __init__(self, df, image_path, transformations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: the dataframe with train or val rows\n",
    "            image_path: path to dir of images\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        # Transforms\n",
    "        self.transforms = transformations\n",
    "        \n",
    "        # store the df\n",
    "        self.data_info = df\n",
    "        \n",
    "        # First column contains the image paths\n",
    "        self.image_arr = self.data_info.iloc[:, 0].values\n",
    "  \n",
    "        # remaining columns are the labels\n",
    "        self.label_arr = self.data_info.iloc[:, 1:].values\n",
    "\n",
    "        # Calculate len\n",
    "        self.data_len = len(self.label_arr)\n",
    "        \n",
    "        # image_path\n",
    "        self.image_path = image_path\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image name from the pandas df\n",
    "        image_name = self.image_arr[index]\n",
    "        \n",
    "        # Open image\n",
    "        # to RGB is needed, some imgs are grayscale\n",
    "        img = Image.open(self.image_path + image_name).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            # do the resize and normalize (based on ImageNet)\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        # Get labels of the image\n",
    "        single_image_label = self.label_arr[index]\n",
    "\n",
    "        return (img, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "188874f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the data we have. To do it we need to load the dataset without augmentations.\n",
    "# read all images names and labels\n",
    "data = pd.read_csv(DATA_CSV, header=None)\n",
    "\n",
    "# split for train, val\n",
    "train_df, val_df = train_test_split(data, test_size=0.2, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e7125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations on Images\n",
    "# for EffNetB4\n",
    "IMAGE_SIDE = 380\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIDE, IMAGE_SIDE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Train preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIDE, IMAGE_SIDE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # added to fight overfitting\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# train and validation datasets\n",
    "train_ds = CustomDatasetFromCSV(train_df, IMAGE_DIR, train_transform)\n",
    "val_ds = CustomDatasetFromCSV(val_df, IMAGE_DIR, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d111cce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 80 classes...\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {N_CLASSES} classes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8112cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class definition\n",
    "# we can choose a different backbone\n",
    "\n",
    "# Use the torchvision's implementation of EFFNetB4\n",
    "# but add FC layer for a different number of classes (27) \n",
    "# and a Sigmoid for multi-label (not exclusive)\n",
    "class EFFNetB4(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        effnetb4 = models.efficientnet_b4(pretrained=True)\n",
    "        effnetb4.classifier[1] = nn.Sequential(nn.Linear(1792, n_classes, bias=True))\n",
    "        \n",
    "        self.base_model = effnetb4\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa98c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataloaders for training.\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_data_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, \n",
    "                               num_workers=NUM_WORKERS, shuffle=True, drop_last=True)\n",
    "val_data_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c894c68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n",
      "We have 78219 samples in train dataset\n",
      "We have 19555 samples in validation dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Datasets:\")\n",
    "print(f\"We have {len(train_ds)} samples in train dataset\")\n",
    "print(f\"We have {len(val_ds)} samples in validation dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e189987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = EFFNetB4(N_CLASSES)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7758709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has: 17692056 trainable params...\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has: {count_parameters(model)} trainable params...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "757e171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(pred, target, threshold=0.5):\n",
    "    pred = np.array(pred > threshold, dtype=float)\n",
    "    \n",
    "    return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc51fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 8\n",
    "# lowered respect to tresnet\n",
    "LR = 2e-4\n",
    "\n",
    "# Loss\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Logs - Helpful for plotting after training finishes\n",
    "train_logs = {\"loss\": [], \"accuracy\": [], \"time\": []}\n",
    "val_logs = {\"loss\": [], \"accuracy\": [], \"time\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae565429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_data_loader):\n",
    "\n",
    "    # Local Parameters\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for images, labels in tqdm(train_data_loader):\n",
    "\n",
    "        # Loading images and labels to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Resetting Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        preds = model(images)\n",
    "\n",
    "        # Calculating Loss\n",
    "        _loss = criterion(preds, labels.type(torch.float))\n",
    "        loss = _loss.item()\n",
    "        epoch_loss.append(loss)\n",
    "\n",
    "        # Backward\n",
    "        _loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Overall Epoch Results \n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Acc and Loss\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "\n",
    "    # Storing results to logs\n",
    "    train_logs[\"loss\"].append(epoch_loss)\n",
    "    train_logs[\"time\"].append(total_time)\n",
    "    \n",
    "    return epoch_loss, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4839ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_one_epoch(val_data_loader):\n",
    "\n",
    "    # Local Parameters\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for images, labels in val_data_loader:\n",
    "\n",
    "        # Loading images and labels to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        preds = model(images)\n",
    "\n",
    "        # Calculating Loss\n",
    "        _loss = criterion(preds, labels.type(torch.float))\n",
    "        loss = _loss.item()\n",
    "        epoch_loss.append(loss)\n",
    "\n",
    "    # Overall Epoch Results\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Acc and Loss\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "\n",
    "    val_logs[\"loss\"].append(epoch_loss)\n",
    "    val_logs[\"time\"].append(total_time)\n",
    "\n",
    "    return epoch_loss, total_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "448024bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1905/9777 [07:50<32:23,  4.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-62e3706f8166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-2bc45dcabb47>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(train_data_loader)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0m_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/computervision_p37_gpu_v1/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/computervision_p37_gpu_v1/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we do the training loop\n",
    "#\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    loss, _time = train_one_epoch(train_data_loader)\n",
    "\n",
    "    print()\n",
    "    print(\n",
    "        f\"Training: Epoch {epoch}, Loss : {round(loss, 4)}, Time : {round(_time, 1)}\"\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    loss, _time, = val_one_epoch(val_data_loader)\n",
    "\n",
    "    print()\n",
    "    print(\n",
    "        f\"Validation: Epoch {epoch}, Loss : {round(loss, 4)}, Time : {round(_time, 1)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "vet_epochs = np.arange(1, EPOCHS+1, 1)\n",
    "\n",
    "# Loss\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(vet_epochs, train_logs[\"loss\"], label=\"Training loss\")\n",
    "plt.plot(vet_epochs, val_logs[\"loss\"], label=\"Validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9aaed",
   "metadata": {},
   "source": [
    "La curva riportata sopra è un chiaro esempio di overfitting: dopo la epoch 4 la training loss continua a diminuire mentre la validation loss comincia a salire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3765ee",
   "metadata": {},
   "source": [
    "#### Compute metrics on the entire validation set (needs to be done on GPU, or is too slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c780e3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"hbku2019.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('hbku2019.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_loader = DataLoader(val_ds, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# reload the model\n",
    "\n",
    "# model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in tqdm(val_data_loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        labels = labels\n",
    "    \n",
    "        # Forward\n",
    "        preds = model(images).to(\"cpu\")\n",
    "    \n",
    "        all_preds.append(preds.numpy())\n",
    "        all_labels.append(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_flatten = [item for sublist in all_preds for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_flatten = [item for sublist in all_labels for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d904e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = calculate_metrics(np.array(all_preds_flatten), np.array(all_labels_flatten))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341ee83",
   "metadata": {},
   "source": [
    "#### Results on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce decimal to 3\n",
    "for key in out.keys():\n",
    "    out[key] = round(out[key], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81200ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7db5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:computervision_p37_gpu_v1]",
   "language": "python",
   "name": "conda-env-computervision_p37_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
